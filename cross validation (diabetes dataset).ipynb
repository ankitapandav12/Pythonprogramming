{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(r\"C:\\Users\\Sushant\\Desktop\\Ankita\\Datasets\\diabetes_dataset.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                 0\n",
       "Glucose                     0\n",
       "BloodPressure               0\n",
       "SkinThickness               0\n",
       "Insulin                     0\n",
       "BMI                         0\n",
       "DiabetesPedigreeFunction    0\n",
       "Age                         0\n",
       "Outcome                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.iloc[:,:-1].values\n",
    "y=data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=4, random_state=10, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=4,random_state=10,shuffle=True)\n",
    "kf.get_n_splits(x)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [  0   2   3   4   5   8   9  10  11  13  14  15  16  17  18  19  20  21\n",
      "  22  23  24  25  26  28  29  30  32  34  36  37  38  40  41  42  44  46\n",
      "  47  48  49  50  51  53  54  55  56  58  59  61  62  63  65  66  67  68\n",
      "  69  70  71  72  73  74  75  76  77  79  81  82  83  86  87  88  89  90\n",
      "  91  93  95  96  97  98  99 101 103 105 106 107 108 109 110 111 112 113\n",
      " 114 115 117 118 119 122 123 124 125 127 128 129 130 133 134 135 136 137\n",
      " 138 139 141 144 145 146 147 148 149 150 151 153 154 155 156 158 159 160\n",
      " 162 164 165 166 168 169 170 174 175 176 177 178 180 182 184 185 186 187\n",
      " 189 198 199 200 201 202 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 219 221 224 225 226 227 228 229 231 232 233 234 235 236 237 238 239\n",
      " 241 243 245 246 247 248 249 250 251 253 255 256 257 258 259 260 261 263\n",
      " 265 266 267 268 269 270 272 275 276 279 281 282 283 284 286 287 288 289\n",
      " 290 291 296 297 300 301 304 305 306 308 310 312 313 314 315 316 317 318\n",
      " 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 336 337 338\n",
      " 339 340 342 343 344 345 346 347 348 350 352 353 355 356 357 358 359 360\n",
      " 361 362 363 364 365 366 367 368 369 370 371 372 373 375 376 377 379 380\n",
      " 382 383 385 388 389 390 391 392 393 394 395 396 397 398 399 401 403 404\n",
      " 406 407 408 409 410 412 415 416 417 419 420 421 422 423 426 427 429 430\n",
      " 432 433 434 436 437 438 439 440 442 445 446 447 449 450 451 452 453 454\n",
      " 455 457 459 460 461 463 464 465 467 468 470 471 472 473 474 477 478 479\n",
      " 480 481 483 488 489 490 491 492 493 494 495 496 498 499 501 502 503 504\n",
      " 505 506 509 511 512 515 516 519 520 521 522 523 525 526 527 528 530 531\n",
      " 532 533 534 535 536 537 540 541 542 543 544 545 546 547 548 550 551 552\n",
      " 553 554 556 557 558 561 565 566 567 569 570 572 574 575 576 578 579 580\n",
      " 581 583 584 585 586 589 591 592 594 595 597 599 600 601 602 606 607 608\n",
      " 610 611 612 616 617 619 622 623 624 625 627 629 630 632 633 634 635 636\n",
      " 637 638 639 640 641 642 643 644 645 646 648 649 650 651 652 653 654 655\n",
      " 656 657 658 659 661 662 663 665 666 667 668 669 670 671 672 673 675 676\n",
      " 678 679 680 681 682 683 685 686 688 689 690 691 692 693 694 695 696 697\n",
      " 698 699 700 702 703 704 705 707 708 709 710 711 712 713 715 716 717 718\n",
      " 722 723 724 728 729 731 732 733 734 735 736 737 738 739 740 741 742 743\n",
      " 744 745 746 747 749 752 753 755 756 757 758 759 760 761 762 764 766 767] TEST: [  1   6   7  12  27  31  33  35  39  43  45  52  57  60  64  78  80  84\n",
      "  85  92  94 100 102 104 116 120 121 126 131 132 140 142 143 152 157 161\n",
      " 163 167 171 172 173 179 181 183 188 190 191 192 193 194 195 196 197 203\n",
      " 217 218 220 222 223 230 240 242 244 252 254 262 264 271 273 274 277 278\n",
      " 280 285 292 293 294 295 298 299 302 303 307 309 311 334 335 341 349 351\n",
      " 354 374 378 381 384 386 387 400 402 405 411 413 414 418 424 425 428 431\n",
      " 435 441 443 444 448 456 458 462 466 469 475 476 482 484 485 486 487 497\n",
      " 500 507 508 510 513 514 517 518 524 529 538 539 549 555 559 560 562 563\n",
      " 564 568 571 573 577 582 587 588 590 593 596 598 603 604 605 609 613 614\n",
      " 615 618 620 621 626 628 631 647 660 664 674 677 684 687 701 706 714 719\n",
      " 720 721 725 726 727 730 748 750 751 754 763 765]\n",
      "TRAIN: [  0   1   2   4   5   6   7   8   9  12  13  15  17  18  19  20  23  25\n",
      "  26  27  29  30  31  32  33  35  36  37  38  39  40  41  42  43  44  45\n",
      "  46  48  49  51  52  53  54  55  57  58  60  62  63  64  66  67  71  73\n",
      "  74  75  77  78  79  80  82  84  85  86  87  89  90  92  93  94  95  99\n",
      " 100 101 102 103 104 106 112 113 116 117 120 121 122 123 124 125 126 127\n",
      " 128 130 131 132 134 135 137 138 140 142 143 144 145 147 148 149 150 152\n",
      " 155 156 157 158 159 161 162 163 165 167 168 170 171 172 173 174 177 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 203 204 205 206 207 208 209 212 213 216 217 218 219 220\n",
      " 221 222 223 224 225 226 227 230 232 234 239 240 242 243 244 246 247 248\n",
      " 250 251 252 254 255 256 258 262 263 264 265 267 268 269 270 271 273 274\n",
      " 275 277 278 280 281 283 285 286 287 288 289 290 291 292 293 294 295 297\n",
      " 298 299 301 302 303 307 308 309 310 311 312 313 314 315 316 319 320 321\n",
      " 324 325 326 328 330 331 332 334 335 336 337 338 339 340 341 342 343 344\n",
      " 346 347 348 349 350 351 352 353 354 355 356 357 359 360 361 362 363 364\n",
      " 365 366 368 369 371 373 374 376 377 378 380 381 382 384 385 386 387 388\n",
      " 389 391 393 394 395 397 400 401 402 404 405 406 409 411 412 413 414 415\n",
      " 416 417 418 420 421 422 423 424 425 426 427 428 430 431 432 434 435 436\n",
      " 438 441 443 444 446 447 448 453 454 455 456 457 458 459 460 461 462 464\n",
      " 465 466 468 469 470 471 472 474 475 476 477 480 482 483 484 485 486 487\n",
      " 490 493 494 496 497 498 500 502 503 504 506 507 508 510 512 513 514 515\n",
      " 516 517 518 519 520 521 522 523 524 526 527 528 529 530 532 533 534 535\n",
      " 536 538 539 540 542 543 545 546 548 549 551 552 553 554 555 556 557 558\n",
      " 559 560 561 562 563 564 568 569 571 572 573 574 575 577 579 580 582 583\n",
      " 584 586 587 588 589 590 591 593 595 596 597 598 600 602 603 604 605 606\n",
      " 607 608 609 610 612 613 614 615 617 618 620 621 624 626 627 628 629 630\n",
      " 631 632 633 634 636 640 644 645 646 647 648 649 650 651 652 653 654 655\n",
      " 656 657 658 660 661 662 663 664 666 668 669 670 671 672 674 676 677 678\n",
      " 680 681 682 683 684 685 687 688 689 691 693 695 696 701 702 703 704 705\n",
      " 706 707 708 709 711 712 713 714 716 718 719 720 721 722 723 724 725 726\n",
      " 727 728 729 730 731 732 733 734 735 736 738 739 741 742 744 745 746 747\n",
      " 748 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 767] TEST: [  3  10  11  14  16  21  22  24  28  34  47  50  56  59  61  65  68  69\n",
      "  70  72  76  81  83  88  91  96  97  98 105 107 108 109 110 111 114 115\n",
      " 118 119 129 133 136 139 141 146 151 153 154 160 164 166 169 175 176 178\n",
      " 202 210 211 214 215 228 229 231 233 235 236 237 238 241 245 249 253 257\n",
      " 259 260 261 266 272 276 279 282 284 296 300 304 305 306 317 318 322 323\n",
      " 327 329 333 345 358 367 370 372 375 379 383 390 392 396 398 399 403 407\n",
      " 408 410 419 429 433 437 439 440 442 445 449 450 451 452 463 467 473 478\n",
      " 479 481 488 489 491 492 495 499 501 505 509 511 525 531 537 541 544 547\n",
      " 550 565 566 567 570 576 578 581 585 592 594 599 601 611 616 619 622 623\n",
      " 625 635 637 638 639 641 642 643 659 665 667 673 675 679 686 690 692 694\n",
      " 697 698 699 700 710 715 717 737 740 743 749 766]\n",
      "TRAIN: [  1   3   4   6   7   8  10  11  12  13  14  15  16  18  21  22  24  27\n",
      "  28  31  32  33  34  35  39  40  41  43  44  45  46  47  48  50  51  52\n",
      "  53  54  56  57  59  60  61  62  64  65  67  68  69  70  71  72  73  74\n",
      "  76  77  78  79  80  81  82  83  84  85  86  88  89  90  91  92  93  94\n",
      "  95  96  97  98 100 102 103 104 105 107 108 109 110 111 114 115 116 118\n",
      " 119 120 121 122 123 125 126 128 129 131 132 133 134 135 136 137 139 140\n",
      " 141 142 143 144 146 150 151 152 153 154 156 157 158 160 161 163 164 166\n",
      " 167 168 169 171 172 173 175 176 178 179 180 181 182 183 185 186 188 190\n",
      " 191 192 193 194 195 196 197 200 201 202 203 206 209 210 211 213 214 215\n",
      " 216 217 218 220 221 222 223 228 229 230 231 232 233 234 235 236 237 238\n",
      " 239 240 241 242 243 244 245 246 249 252 253 254 255 256 257 258 259 260\n",
      " 261 262 263 264 265 266 268 271 272 273 274 276 277 278 279 280 282 283\n",
      " 284 285 286 290 292 293 294 295 296 298 299 300 302 303 304 305 306 307\n",
      " 309 311 313 317 318 319 320 321 322 323 327 328 329 330 333 334 335 337\n",
      " 338 340 341 342 344 345 346 347 349 350 351 354 355 356 357 358 360 363\n",
      " 364 366 367 369 370 371 372 373 374 375 376 378 379 380 381 382 383 384\n",
      " 385 386 387 390 392 393 395 396 398 399 400 401 402 403 405 406 407 408\n",
      " 409 410 411 412 413 414 416 418 419 420 421 422 424 425 427 428 429 430\n",
      " 431 433 434 435 436 437 439 440 441 442 443 444 445 446 448 449 450 451\n",
      " 452 453 454 456 458 459 461 462 463 464 465 466 467 468 469 470 472 473\n",
      " 475 476 478 479 480 481 482 484 485 486 487 488 489 490 491 492 495 496\n",
      " 497 498 499 500 501 502 503 505 506 507 508 509 510 511 513 514 515 516\n",
      " 517 518 521 523 524 525 526 527 528 529 530 531 534 535 537 538 539 540\n",
      " 541 542 543 544 545 547 549 550 551 554 555 556 557 559 560 562 563 564\n",
      " 565 566 567 568 570 571 573 574 576 577 578 581 582 583 585 587 588 589\n",
      " 590 591 592 593 594 596 597 598 599 600 601 603 604 605 606 607 608 609\n",
      " 611 612 613 614 615 616 618 619 620 621 622 623 624 625 626 627 628 630\n",
      " 631 632 635 636 637 638 639 640 641 642 643 645 647 649 652 653 655 657\n",
      " 659 660 662 664 665 667 668 670 671 673 674 675 677 679 683 684 685 686\n",
      " 687 689 690 691 692 694 696 697 698 699 700 701 706 708 709 710 711 712\n",
      " 714 715 717 718 719 720 721 723 725 726 727 730 733 734 735 736 737 740\n",
      " 743 744 745 746 747 748 749 750 751 753 754 755 757 759 762 763 765 766] TEST: [  0   2   5   9  17  19  20  23  25  26  29  30  36  37  38  42  49  55\n",
      "  58  63  66  75  87  99 101 106 112 113 117 124 127 130 138 145 147 148\n",
      " 149 155 159 162 165 170 174 177 184 187 189 198 199 204 205 207 208 212\n",
      " 219 224 225 226 227 247 248 250 251 267 269 270 275 281 287 288 289 291\n",
      " 297 301 308 310 312 314 315 316 324 325 326 331 332 336 339 343 348 352\n",
      " 353 359 361 362 365 368 377 388 389 391 394 397 404 415 417 423 426 432\n",
      " 438 447 455 457 460 471 474 477 483 493 494 504 512 519 520 522 532 533\n",
      " 536 546 548 552 553 558 561 569 572 575 579 580 584 586 595 602 610 617\n",
      " 629 633 634 644 646 648 650 651 654 656 658 661 663 666 669 672 676 678\n",
      " 680 681 682 688 693 695 702 703 704 705 707 713 716 722 724 728 729 731\n",
      " 732 738 739 741 742 752 756 758 760 761 764 767]\n",
      "TRAIN: [  0   1   2   3   5   6   7   9  10  11  12  14  16  17  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  33  34  35  36  37  38  39  42  43\n",
      "  45  47  49  50  52  55  56  57  58  59  60  61  63  64  65  66  68  69\n",
      "  70  72  75  76  78  80  81  83  84  85  87  88  91  92  94  96  97  98\n",
      "  99 100 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117\n",
      " 118 119 120 121 124 126 127 129 130 131 132 133 136 138 139 140 141 142\n",
      " 143 145 146 147 148 149 151 152 153 154 155 157 159 160 161 162 163 164\n",
      " 165 166 167 169 170 171 172 173 174 175 176 177 178 179 181 183 184 187\n",
      " 188 189 190 191 192 193 194 195 196 197 198 199 202 203 204 205 207 208\n",
      " 210 211 212 214 215 217 218 219 220 222 223 224 225 226 227 228 229 230\n",
      " 231 233 235 236 237 238 240 241 242 244 245 247 248 249 250 251 252 253\n",
      " 254 257 259 260 261 262 264 266 267 269 270 271 272 273 274 275 276 277\n",
      " 278 279 280 281 282 284 285 287 288 289 291 292 293 294 295 296 297 298\n",
      " 299 300 301 302 303 304 305 306 307 308 309 310 311 312 314 315 316 317\n",
      " 318 322 323 324 325 326 327 329 331 332 333 334 335 336 339 341 343 345\n",
      " 348 349 351 352 353 354 358 359 361 362 365 367 368 370 372 374 375 377\n",
      " 378 379 381 383 384 386 387 388 389 390 391 392 394 396 397 398 399 400\n",
      " 402 403 404 405 407 408 410 411 413 414 415 417 418 419 423 424 425 426\n",
      " 428 429 431 432 433 435 437 438 439 440 441 442 443 444 445 447 448 449\n",
      " 450 451 452 455 456 457 458 460 462 463 466 467 469 471 473 474 475 476\n",
      " 477 478 479 481 482 483 484 485 486 487 488 489 491 492 493 494 495 497\n",
      " 499 500 501 504 505 507 508 509 510 511 512 513 514 517 518 519 520 522\n",
      " 524 525 529 531 532 533 536 537 538 539 541 544 546 547 548 549 550 552\n",
      " 553 555 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573\n",
      " 575 576 577 578 579 580 581 582 584 585 586 587 588 590 592 593 594 595\n",
      " 596 598 599 601 602 603 604 605 609 610 611 613 614 615 616 617 618 619\n",
      " 620 621 622 623 625 626 628 629 631 633 634 635 637 638 639 641 642 643\n",
      " 644 646 647 648 650 651 654 656 658 659 660 661 663 664 665 666 667 669\n",
      " 672 673 674 675 676 677 678 679 680 681 682 684 686 687 688 690 692 693\n",
      " 694 695 697 698 699 700 701 702 703 704 705 706 707 710 713 714 715 716\n",
      " 717 719 720 721 722 724 725 726 727 728 729 730 731 732 737 738 739 740\n",
      " 741 742 743 748 749 750 751 752 754 756 758 760 761 763 764 765 766 767] TEST: [  4   8  13  15  18  32  40  41  44  46  48  51  53  54  62  67  71  73\n",
      "  74  77  79  82  86  89  90  93  95 103 122 123 125 128 134 135 137 144\n",
      " 150 156 158 168 180 182 185 186 200 201 206 209 213 216 221 232 234 239\n",
      " 243 246 255 256 258 263 265 268 283 286 290 313 319 320 321 328 330 337\n",
      " 338 340 342 344 346 347 350 355 356 357 360 363 364 366 369 371 373 376\n",
      " 380 382 385 393 395 401 406 409 412 416 420 421 422 427 430 434 436 446\n",
      " 453 454 459 461 464 465 468 470 472 480 490 496 498 502 503 506 515 516\n",
      " 521 523 526 527 528 530 534 535 540 542 543 545 551 554 556 557 574 583\n",
      " 589 591 597 600 606 607 608 612 624 627 630 632 636 640 645 649 652 653\n",
      " 655 657 662 668 670 671 683 685 689 691 696 708 709 711 712 718 723 733\n",
      " 734 735 736 744 745 746 747 753 755 757 759 762]\n"
     ]
    }
   ],
   "source": [
    "for train_index,test_index in kf.split(x):\n",
    "    print(\"TRAIN:\",train_index,\"TEST:\",test_index)\n",
    "    x_train,x_test=x[train_index],x[test_index]\n",
    "    y_train,y_test=y[train_index],y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72222222 0.79166667 0.79861111 0.77777778]\n",
      "77.25694444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sushant\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "C:\\Users\\Sushant\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "C:\\Users\\Sushant\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "C:\\Users\\Sushant\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "l1=LogisticRegression()\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores=cross_val_score(l1,x_train,y_train,cv=kf)\n",
    "print(scores)\n",
    "print(np.mean(scores)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
      " 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1\n",
      " 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 1 0 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sushant\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "C:\\Users\\Sushant\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "C:\\Users\\Sushant\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "C:\\Users\\Sushant\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_pred=cross_val_predict(l1,x_test,y_test,cv=kf)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
